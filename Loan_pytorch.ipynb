{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1NB1BhsKe--88XnRJS2QCjMs96RFHl6RF",
      "authorship_tag": "ABX9TyOJGn8y7mTRBaQM6yszK1sM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahcho124/AI-deepfake-audio-detction/blob/main/Loan_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "TSSo_vAviC8h",
        "outputId": "41269cba-a59d-4fa7-c7d9-d87d0542d6ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No samples will be generated with the provided ratio settings.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4df212518e94>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Split the data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Convert NumPy arrays to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m         )\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         y_ = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/over_sampling/_adasyn.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    196\u001b[0m                     \u001b[0;34m\"No samples will be generated with the provided ratio settings.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: No samples will be generated with the provided ratio settings."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "\n",
        "def feature_tuning(train):\n",
        "    train.drop(['대출목적', '근로기간'], axis = 1, inplace= True)\n",
        "    train.loc[train['대출기간'] == ' 36 months','DTI'] = train['연간소득'] / (train['대출금액'] / 36)\n",
        "    train.loc[train['대출기간'] == ' 60 months','DTI'] = train['연간소득'] / (train['대출금액'] / 60)\n",
        "    train['이자율'] = (train['총상환이자']+10) / (train['총상환원금']+10)\n",
        "    train.loc[train['총연체금액'] < 30000, '이자율'] = 0.4\n",
        "    train['연간소득'] = np.log(train['연간소득'] + 0.1)\n",
        "    train.loc[train['총연체금액'] > 0, '총연체금액'] = True\n",
        "    train.loc[train['총연체금액'] == 0, '총연체금액'] = False\n",
        "    # train['근로기간'] = train['근로기간'].replace(work_dict)\n",
        "    # cats = train['대출목적'].value_counts()[lambda x: x<100].index # 대출목적 column에서 value가 100개 넘는 feature에 대해서만 dummy 생성\n",
        "    # train['대출목적'].replace(cats[[i for i in range(len(cats))]], '기타', inplace = True)\n",
        "    cats = train['주택소유상태'].value_counts()[lambda x: x<100].index # 주택소유상태 column에서 value가 100개 넘는 feature에 대해서만 dummy 생성\n",
        "    train['주택소유상태'].replace(cats[[i for i in range(len(cats))]], None, inplace = True)\n",
        "    train = pd.get_dummies(train, columns= ['대출기간','주택소유상태']) # 가변수 지정 여부 바꿀수도 있음. drop_first = True\n",
        "    train = train.set_index('ID')\n",
        "    return train\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "target = train['대출등급']\n",
        "train.drop('대출등급', axis = 1, inplace=True)\n",
        "train = feature_tuning(train)\n",
        "\n",
        "imb = ADASYN()\n",
        "scaler = RobustScaler()\n",
        "label_encoder = LabelEncoder()\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# train_x_scaled = scaler.fit_transform(train)\n",
        "target_encoded = label_encoder.fit_transform(target)\n",
        "target_encoded = onehot_encoder.fit_transform(target_encoded.reshape(-1,1))\n",
        "# Split the data into training and validation sets\n",
        "train_data, valid_data, train_target, valid_target = train_test_split(train, target_encoded, test_size=0.2, stratify=target_encoded)\n",
        "train_data, train_target = imb.fit_resample(train_data, train_target)\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
        "train_target_tensor = torch.tensor(train_target, dtype=torch.long)\n",
        "valid_data_tensor = torch.tensor(valid_data, dtype=torch.float32)\n",
        "valid_target_tensor = torch.tensor(valid_target, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_dataset = TensorDataset(train_data_tensor, train_target_tensor)\n",
        "valid_dataset = TensorDataset(valid_data_tensor, valid_target_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=200, shuffle=False)\n",
        "\n",
        "# Define the neural network model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 20)\n",
        "        self.fc2 = nn.Linear(20, 16)\n",
        "        self.fc3 = nn.Linear(16, 10)\n",
        "        # self.fc4 = nn.Linear(50, 35)\n",
        "        # self.fc5 = nn.Linear(35, 20)\n",
        "        self.output_layer = nn.Linear(10, num_classes)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        # x = self.activation(self.fc4(x))\n",
        "        # x = self.activation(self.fc5(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "num_classes = len(set(target))\n",
        "input_size = train_data.shape[1]\n",
        "model = MLP(input_size)\n",
        "\n",
        "# Move the model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model.forward(data)\n",
        "        target = torch.tensor(target, dtype= torch.float32)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_data_tensor = valid_data_tensor.to(device)\n",
        "        valid_target_tensor = valid_target_tensor.to(device)\n",
        "        outputs = model(valid_data_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predicted_cpu = predicted.cpu().numpy()\n",
        "        predicted = onehot_encoder.transform(predicted_cpu.reshape(-1,1))\n",
        "        # predicted = torch.argmax(outputs,1)\n",
        "        # valid_target_tensor = torch.argmax(valid_target_tensor,1)\n",
        "        accuracy = accuracy_score(valid_target_tensor.cpu().numpy(), predicted)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Test the model on the entire dataset\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    train_data_tensor = train_data_tensor.to(device)\n",
        "    train_target_tensor = train_target_tensor.to(device)\n",
        "    outputs = model(train_data_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predicted_cpu = predicted.cpu().numpy()\n",
        "    predicted = onehot_encoder.transform(predicted_cpu.reshape(-1,1))\n",
        "    accuracy = accuracy_score(train_target_tensor.cpu().numpy(), predicted)\n",
        "    print(f'Training Accuracy: {accuracy:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print('Results on the test set:')\n",
        "print(classification_report(train_target_tensor.cpu().numpy(), predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8_NryH-k_nW",
        "outputId": "c68fd306-da03-4ea2-fd0d-eac47b43650f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on the test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90     23054\n",
            "           1       0.88      0.79      0.83     23054\n",
            "           2       0.81      0.87      0.84     23054\n",
            "           3       0.85      0.84      0.84     23054\n",
            "           4       0.82      0.84      0.83     23054\n",
            "           5       0.84      0.89      0.86     23054\n",
            "           6       0.97      0.87      0.92     23054\n",
            "\n",
            "   micro avg       0.86      0.86      0.86    161378\n",
            "   macro avg       0.86      0.86      0.86    161378\n",
            "weighted avg       0.86      0.86      0.86    161378\n",
            " samples avg       0.86      0.86      0.86    161378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "test = feature_tuning(test)\n",
        "test = scaler.transform(test)\n",
        "test = torch.tensor(test, dtype=torch.float32)\n",
        "test = test.to('cuda')\n",
        "model.to('cuda')\n",
        "prediction = model.forward(test)\n",
        "_, prediction = torch.max(prediction, 1)\n",
        "predicted_cpu = prediction.cpu().numpy()\n",
        "prediction = onehot_encoder.transform(predicted_cpu.reshape(-1,1))\n",
        "decoded_pred = onehot_encoder.inverse_transform(prediction)\n",
        "decode_dict = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F',6:'G'}\n",
        "prediction = np.vectorize(decode_dict.get)(decoded_pred)"
      ],
      "metadata": {
        "id": "J4LNCZOpi2em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv('/content/drive/MyDrive/sample_submission.csv')\n",
        "sample_submission['대출등급'] = prediction\n",
        "sample_submission.to_csv('submission_smote.csv', index=None)"
      ],
      "metadata": {
        "id": "8oO5UWCIlEwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def feature_tuning(train):\n",
        "    train.drop(['대출목적', '근로기간'], axis = 1, inplace= True)\n",
        "    train.loc[train['대출기간'] == ' 36 months','DTI'] = train['연간소득'] / (train['대출금액'] / 36)\n",
        "    train.loc[train['대출기간'] == ' 60 months','DTI'] = train['연간소득'] / (train['대출금액'] / 60)\n",
        "    train['이자율'] = (train['총상환이자']+10) / (train['총상환원금']+10)\n",
        "    train.loc[train['총연체금액'] < 30000, '이자율'] = 0.4\n",
        "    train['연간소득'] = np.log(train['연간소득'] + 0.1)\n",
        "    train.loc[train['총연체금액'] > 0, '총연체금액'] = True\n",
        "    train.loc[train['총연체금액'] == 0, '총연체금액'] = False\n",
        "    # train['근로기간'] = train['근로기간'].replace(work_dict)\n",
        "    # cats = train['대출목적'].value_counts()[lambda x: x<100].index # 대출목적 column에서 value가 100개 넘는 feature에 대해서만 dummy 생성\n",
        "    # train['대출목적'].replace(cats[[i for i in range(len(cats))]], '기타', inplace = True)\n",
        "    cats = train['주택소유상태'].value_counts()[lambda x: x<100].index # 주택소유상태 column에서 value가 100개 넘는 feature에 대해서만 dummy 생성\n",
        "    train['주택소유상태'].replace(cats[[i for i in range(len(cats))]], None, inplace = True)\n",
        "    train = pd.get_dummies(train, columns= ['대출기간','주택소유상태']) # 가변수 지정 여부 바꿀수도 있음. drop_first = True\n",
        "    train = train.set_index('ID')\n",
        "    return train\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "target = train['대출등급']\n",
        "train.drop('대출등급', axis = 1, inplace=True)\n",
        "train = feature_tuning(train)\n",
        "\n",
        "# train, validation data set 분리\n",
        "x_train , x_valid, y_train, y_valid = train_test_split(train, target, test_size=0.2, shuffle=True, stratify = target, random_state=42)\n",
        "sm = SMOTE()\n",
        "scaler = StandardScaler()\n",
        "train_x_resampled, train_y_resampled = sm.fit_resample(train, target)\n",
        "train_x_scaled = scaler.fit_transform(train_x_resampled)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(train_x_scaled, train_y_resampled)\n",
        "prediction = lr.predict(x_valid)\n",
        "from sklearn.metrics import classification_report\n",
        "print('Results on the test set:')\n",
        "print(classification_report(y_valid, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtMUKq_0ph8b",
        "outputId": "8a085b3d-2d62-4361-d9b3-c66888c8cc19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on the test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.22      1.00      0.36      3354\n",
            "           B       0.00      0.00      0.00      5763\n",
            "           C       0.00      0.00      0.00      5525\n",
            "           D       0.33      0.00      0.00      2671\n",
            "           E       0.26      0.68      0.38      1471\n",
            "           F       0.58      0.19      0.28       391\n",
            "           G       0.00      0.00      0.00        84\n",
            "\n",
            "    accuracy                           0.23     19259\n",
            "   macro avg       0.20      0.27      0.15     19259\n",
            "weighted avg       0.12      0.23      0.10     19259\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vl6xsndTqLMu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}